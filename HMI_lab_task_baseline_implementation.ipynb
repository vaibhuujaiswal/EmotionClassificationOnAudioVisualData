{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HMI_lab_task_baseline_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMY/7jJyjEtjUj3VtD4HaLO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhuujaiswal/EmotionClassificationOnAudioVisualData/blob/main/HMI_lab_task_baseline_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connecting drive with google collab to transfer audio visual dataset"
      ],
      "metadata": {
        "id": "1tAZ-YRpgD0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('../drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkfuCul66EEi",
        "outputId": "cbcf43d1-15c7-45a5-d764-7c1e3db3f549"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at ../drive; to attempt to forcibly remount, call drive.mount(\"../drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries to be imported :"
      ],
      "metadata": {
        "id": "vS7Xf4vHf6RP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "iayI-DczO1R2"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import imutils\n",
        "import dlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix #for metrics\n",
        "import pandas as pd\n",
        "import librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detection through video :"
      ],
      "metadata": {
        "id": "PagT1seKnPnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LBP implementation after which features are extracted from non-overlapping spatial 4*4 blocks!"
      ],
      "metadata": {
        "id": "iJvAeHbEgRUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_pixel(image, center, x , y):\n",
        "    new_value = 0\n",
        "    try:\n",
        "        if image[x][y] >= center:\n",
        "            new_value = 1\n",
        "    except:\n",
        "        pass\n",
        "    return new_value\n",
        "\n",
        "def lbp_calculated_pixel(image, x, y):\n",
        "\tcenter = image[x][y]\n",
        "\tval_ar = []\n",
        "\tval_ar.append(get_pixel(image, center, x-1, y-1))\n",
        "\tval_ar.append(get_pixel(image, center, x-1, y))\n",
        "\tval_ar.append(get_pixel(image, center, x-1, y + 1))\n",
        "\tval_ar.append(get_pixel(image, center, x, y + 1))\n",
        "\tval_ar.append(get_pixel(image, center, x + 1, y + 1))\n",
        "\tval_ar.append(get_pixel(image, center, x + 1, y))\n",
        "\tval_ar.append(get_pixel(image, center, x + 1, y-1))\n",
        "\tval_ar.append(get_pixel(image, center, x, y-1))\n",
        "\tpower_val= [1, 2, 4, 8, 16, 32, 64, 128]\n",
        "\tval = 0\n",
        "\tfor i in range(len(val_ar)):\n",
        "\t\tval += val_ar[i] * power_val[i]\n",
        "\n",
        "\treturn val\n",
        "\n",
        "def split(array, nrows, ncols):\n",
        "    r, h = array.shape\n",
        "    return array.reshape(h//nrows, nrows, -1, ncols).swapaxes(1, 2).reshape(-1, nrows, ncols)\n",
        "\n",
        "def returnAVG(originalMatrix):\n",
        "  avg= []\n",
        "  for i in split(originalMatrix, 4, 4):\n",
        "    avg.append(np.mean(i))\n",
        "  return avg\n",
        "\n",
        "def getFrames(fileName):\n",
        "  capture= cv2.VideoCapture(fileName)\n",
        "  frameNr= 0\n",
        "  frame_list= []\n",
        "  while (True):\n",
        "      if frameNr % 100 == 0:\n",
        "        pass\n",
        "      if frameNr > 1440:\n",
        "        break\n",
        "      success, frame= capture.read()\n",
        "      if success:\n",
        "          frame= cv2.resize(frame,(120,120))\n",
        "          frame_list.append(frame)\n",
        "      else:\n",
        "          break\n",
        "      frameNr += 1\n",
        "  capture.release()\n",
        "  tempFrames= []\n",
        "  for i in range(0, len(frame_list), len(frame_list)//4+1):\n",
        "    tempFrames.append(frame_list[i])\n",
        "  return tempFrames"
      ],
      "metadata": {
        "id": "No_e5uIzO8k4"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction"
      ],
      "metadata": {
        "id": "lCE0H4Pk-mSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataAverage = []\n",
        "changedDataAverage = []\n",
        "path = os.getcwd()\n",
        "print(path)\n",
        "\n",
        "for actor in os.listdir('/drive/MyDrive/HMI_code_task/audio_video/'):\n",
        "    # with ZipFile('./drive/MyDrive/HMI_code_task/audio-video/', 'r') as zipObj:\n",
        "    #     zipObj.extractall()\n",
        "        \n",
        "    for video in os.listdir(f'/drive/MyDrive/HMI_code_task/audio_video/{actor}/'):\n",
        "        if video == '.DS_Store': #handles uploading case for macbook\n",
        "            continue\n",
        "        if int(video[-6:-4]) <= 20 and int(video[0:2]) == 1 and video.find('.mp4') != -1: #Using the first twenty actors for training and choosing from video\n",
        "            print(video)\n",
        "            videoName = f\"/drive/MyDrive/HMI_code_task/audio_video/{actor}/{video}\"\n",
        "\n",
        "            try:\n",
        "                listFrames= getFrames(videoName)\n",
        "                videoAVGs= []\n",
        "                for eachFrame in listFrames:\n",
        "                    gray= cv2.cvtColor(eachFrame, cv2.COLOR_BGR2GRAY) #convert frame to grayscale\n",
        "                    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\") #using opencv's haarcascade function\n",
        "                    #fer could also have been used for the same.\n",
        "                    faces = faceCascade.detectMultiScale(gray, scaleFactor = 1.05, minNeighbors = 6, minSize=(30, 30))\n",
        "                    \n",
        "                    image_frame = None\n",
        "                    for (x, y, w, h) in faces:\n",
        "                        cv2.rectangle(eachFrame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "                        image_frame = eachFrame[y:y + h, x:x + w]\n",
        "\n",
        "                    #resize image to 128 * 128 \n",
        "                    color_frame_bgr = image_frame\n",
        "                    color_frame_bgr = cv2.resize(color_frame_bgr, (128, 128))\n",
        "                    \n",
        "                    #convert image to black and white\n",
        "                    height, width, _= color_frame_bgr.shape\n",
        "                    gray_frame= cv2.cvtColor(color_frame_bgr, cv2.COLOR_BGR2GRAY)\n",
        "                    frame_LBP = np.zeros((height, width), np.uint8)\n",
        "                    \n",
        "                    #Post aligning Local Binary Pattern\n",
        "                    for i in range(0, height):\n",
        "                        for j in range(0, width):\n",
        "                            frame_LBP[i, j] = lbp_calculated_pixel(gray_frame, i, j)\n",
        "                    \n",
        "                    videoAVGs.append(returnAVG(frame_LBP))\n",
        "\n",
        "                dataAverage.append(videoAVGs)\n",
        "                changedDataAverage.append(video[6:8])\n",
        "            except Exception as E:\n",
        "                print(\"Error in feature extraction\")\n",
        "                pass\n",
        "\n",
        "\n",
        "dataAverage = np.array(dataAverage)\n",
        "dataAverage = np.reshape(dataAverage, (-1, 4*1024))\n",
        "dataAverage = dataAverage[:1200,:]\n",
        "dataAverage.shape\n",
        "\n",
        "changedDataAverage = np.array(changedDataAverage)\n",
        "changedDataAverage = changedDataAverage[:1200,]\n",
        "changedDataAverage.shape\n",
        "\n",
        "baseline_video_train_x = dataAverage[:1200,]\n",
        "baseline_video_train_y= changedDataAverage[:1200,]\n"
      ],
      "metadata": {
        "id": "0DDLDM1NSZJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM is learned for classification :"
      ],
      "metadata": {
        "id": "vNmCNpvaMKAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataAverage = []\n",
        "changedDataAverage = []\n",
        "\n",
        "for actor in os.listdir('/drive/MyDrive/HMI_code_task/audio_video/'):\n",
        "  for video in os.listdir(f'/drive/MyDrive/HMI_code_task/audio_video/{actor}'):\n",
        "    if video == '.DS_Store':\n",
        "            continue\n",
        "    if int(video[-6:-4]) > 20 and video[0:2] == '01' and video.find('.mp4') != -1:\n",
        "      videoName= f\"/drive/MyDrive/HMI_code_task/audio_video/{actor}/{video}\"\n",
        "      print(video)\n",
        "      try:\n",
        "        listFrames = getFrames(videoName)\n",
        "        videoAVGs = []\n",
        "        for eachFrame in listFrames:\n",
        "            gray= cv2.cvtColor(eachFrame, cv2.COLOR_BGR2GRAY)\n",
        "            faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "            faces = faceCascade.detectMultiScale(gray, scaleFactor=1.05,minNeighbors=6, minSize=(30, 30))\n",
        "            \n",
        "            image_frame= None\n",
        "            for (x, y, w, h) in faces:\n",
        "                cv2.rectangle(eachFrame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "                image_frame= eachFrame[y:y + h, x:x + w]\n",
        "\n",
        "            color_frame_bgr= image_frame\n",
        "            color_frame_bgr= cv2.resize(color_frame_bgr, (128, 128))\n",
        "            \n",
        "            height, width, _= color_frame_bgr.shape\n",
        "            gray_frame= cv2.cvtColor(color_frame_bgr, cv2.COLOR_BGR2GRAY)\n",
        "            frame_LBP= np.zeros((height, width), np.uint8)\n",
        "            \n",
        "            for i in range(0, height):\n",
        "              for j in range(0, width):\n",
        "                frame_LBP[i, j]= lbp_calculated_pixel(gray_frame, i, j)\n",
        "            \n",
        "            videoAVGs.append(returnAVG(frame_LBP))\n",
        "        dataAverage.append(videoAVGs)\n",
        "        changedDataAverage.append(video[6:8])\n",
        "      except Exception as E:\n",
        "        pass\n",
        "\n",
        "\n",
        "dataAverage= np.array(dataAverage)\n",
        "dataAverage= np.reshape(dataAverage, (-1, 4*1024))\n",
        "dataAverage= dataAverage[:240,:]\n",
        "dataAverage.shape\n",
        "\n",
        "changedDataAverage = np.array(changedDataAverage)\n",
        "changedDataAverage = changedDataAverage[:240,]\n",
        "changedDataAverage.shape\n",
        "\n",
        "baseline_video_test_x = dataAverage\n",
        "baseline_video_test_y= changedDataAverage"
      ],
      "metadata": {
        "id": "o37Nn3TdLYHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_testVideo, x_test_testVideo, y_train_testVideo, y_test_testVideo = train_test_split(dataAverage, changedDataAverage, test_size= 0.2, shuffle= True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_UpxbQ9Xy7K",
        "outputId": "1bd78ef2-7485-4f85-d444-8159bd811426"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[255.    255.    255.    ... 143.    143.    164.   ]\n",
            " [255.    255.    255.    ... 143.    141.5   164.   ]\n",
            " [255.    255.    255.    ... 143.    141.5   164.   ]\n",
            " ...\n",
            " [255.    255.    255.    ... 143.    143.    164.   ]\n",
            " [255.    255.    254.    ...  82.625 108.25  171.75 ]\n",
            " [255.    255.    255.    ... 200.    219.5   109.75 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimension reduction using PCA"
      ],
      "metadata": {
        "id": "jRVRVAVjYAOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca= PCA(n_components= 30)\n",
        "x_train= pca.fit_transform(baseline_video_train_x)\n",
        "x_test= pca.transform(baseline_video_test_x)"
      ],
      "metadata": {
        "id": "ZoOrHPmAX2ud"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model *Training*"
      ],
      "metadata": {
        "id": "WjLGWlweYD_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC()\n",
        "svm.fit(x_train, baseline_video_train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozst1_V-X7xZ",
        "outputId": "b78116e2-8631-4c9e-c5c7-bab87f4a379c"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM metrics (using SVM skit learn)"
      ],
      "metadata": {
        "id": "Om3jW0DxYxSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_Pred = svm.predict(x_test)\n",
        "print(\"SVM\")\n",
        "print(\"SVM Testing Accuracy : \", accuracy_score(baseline_video_test_y, y_Pred)*100)\n",
        "print(\"SVM Precision Score : \", precision_score(baseline_video_test_y, y_Pred, average='macro')*100)\n",
        "print(\"SVM Recall Score : \", recall_score(baseline_video_test_y, y_Pred, average='macro')*100)\n",
        "print(\"SVM F1 Score : \", f1_score(baseline_video_test_y, y_Pred, average='macro'))\n",
        "print(\"SVM Confusion Matrix\\n\", confusion_matrix(baseline_video_test_y, y_Pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEJlI4DNY1U7",
        "outputId": "5c2cb210-d62f-4ab0-a535-22019ab2e6fe"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM\n",
            "SVM Testing Accuracy :  27.350427350427353\n",
            "SVM Precision Score :  28.62251905015063\n",
            "SVM Recall Score :  25.942887931034488\n",
            "SVM F1 Score :  0.23631163979864725\n",
            "SVM Confusion Matrix\n",
            " [[ 2 11  0  0  0  0  0  3]\n",
            " [ 0 21  6  1  1  0  0  3]\n",
            " [ 0 11 21  0  0  0  0  0]\n",
            " [ 3  9  2  0  5  2  0  8]\n",
            " [ 4  7  1  0  5 10  0  5]\n",
            " [ 4  9  0  2  1  7  1  8]\n",
            " [ 5  3  0  0  6  7  4  7]\n",
            " [ 6 10  0  0  1  7  1  4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##AUDIO through smile. \n",
        "\n",
        "Please note that openEAR was no longer available for using. The github repository has not been present and maybe has been removed due to licensing issues. Opensmile.git can be used for the same.\n",
        "\n",
        "Installation with help of : https://audeering.github.io/opensmile/get-started.html"
      ],
      "metadata": {
        "id": "91jOxAmbbFh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Moviepy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmSuRNS7lvnk",
        "outputId": "02a351a2-6bcf-4cc9-95f4-cf30a2402b45"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Moviepy in /usr/local/lib/python3.7/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from Moviepy) (2.4.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from Moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from Moviepy) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from Moviepy) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->Moviepy) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"https://towardsdatascience.com/speech-emotion-recognition-using-ravdess-audio-dataset-ce19d162690#:~:text=It%20is%20a%20system%20through,field%20or%20customer%20call%20centers.\"\n",
        "\n",
        "\n",
        "emotionList = []\n",
        "actorList = []\n",
        "file_path = []\n",
        "\n",
        "for actor in os.listdir('/drive/MyDrive/HMI_code_task/audio_video/'):\n",
        "    for video in os.listdir(f'/drive/MyDrive/HMI_code_task/audio_video/{actor}/'):\n",
        "        if video == '.DS_Store':\n",
        "            continue\n",
        "        if video[0:2] == '01' and video.find(\".mp4\") != -1:\n",
        "            filename= f\"/drive/MyDrive/HMI_code_task/audio_video/{actor}/{video}\"\n",
        "            part = filename.split('.')[0].split('-')\n",
        "            emotionList.append(int(part[2]))\n",
        "            actorList.append(int(part[6]))\n",
        "            file_path.append(f\"/drive/MyDrive/HMI_code_task/audio_video/{actor}/{video}\")\n",
        "\n",
        "\n",
        "\n",
        "audio_df = pd.DataFrame(emotionList)\n",
        "audio_df = audio_df.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n",
        "audio_df = pd.concat([audio_df, pd.DataFrame(actorList)], axis=1)\n",
        "audio_df.columns = ['emotion','actor']\n",
        "audio_df = pd.concat([audio_df,pd.DataFrame(file_path, columns = ['path'])],axis=1)\n"
      ],
      "metadata": {
        "id": "f-xxIc5d9QPX"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "df = pd.DataFrame(columns=['mel_spectrogram'])\n",
        "counter = 0\n",
        "for index, path in enumerate(audio_df.path):\n",
        "    X, sample_rate = librosa.load(path, res_type = 'kaiser_fast',duration = 3,sr = 44100,offset = 0.5)\n",
        "    print(path)\n",
        "    spectrogram = librosa.feature.melspectrogram(y=X, sr=sample_rate, n_mels=128,fmax=8000) \n",
        "    db_spec = librosa.power_to_db(spectrogram)\n",
        "    log_spectrogram = np.mean(db_spec, axis = 0)\n",
        "    \n",
        "    df.loc[counter] = [log_spectrogram]\n",
        "    counter=counter+1\n",
        "print(len(df))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "edmSL6k2-e5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train,test = train_test_split(df_combined, test_size=0.2, random_state=0,\n",
        "                               stratify = df_combined[['emotion','actor']])\n",
        "\n",
        "X_train = train.iloc[:, 3:]\n",
        "y_train = train.iloc[:,:2]\n",
        "X_test = test.iloc[:,3:]\n",
        "y_test = test.iloc[:,:2]"
      ],
      "metadata": {
        "id": "-WK8v8qh-pBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}